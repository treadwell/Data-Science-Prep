---
title: "SVM (Polynomial Kernel) Learning Curve Analysis of Perfect Order Data"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: pdf_document
---

To Do
-----
* Change data set read to svmRadial

```{r packages, warning=FALSE}
# Load necessary packages 

library(tidyr)
library(microbenchmark)
library(plyr)
library(dplyr)
library(lattice)
library(ggplot2)
library(caret)
library(AppliedPredictiveModeling)
#library(rpart)
#library(nnet)
#library(neuralnet)
#library(gbm)
library(kernlab)
```

```{r data, echo=FALSE}
# Clear workspace and set working directory
rm(list = ls())
setwd("/Users/kbrooks/Documents/CS7641 Machine Learning/Assignment 1")

modelTune = readRDS("Perfect_Order_SVM_poly_modelTune.rds") # change this
d = readRDS("Perfect_Order_d.rds")
training = readRDS("Perfect_Order_training.rds")
testing = readRDS("Perfect_Order_testing.rds")

set.seed(250)
#summary(d)

```

Extract Parameters from Tuned Model
-----------------------------------

```{r calcLC, echo=FALSE, warning=FALSE}

degree = modelTune$bestTune$degree
scale = modelTune$bestTune$scale
C = modelTune$bestTune$C

cvCtrl = trainControl(method = "repeatedcv", repeats = 3)
search.grid <- expand.grid(.degree = degree,
                           .scale = scale,
                           .C = C)

cat("Total observations:", nrow(d))
cat("Training observations:", nrow(training))
cat("Testing observations:", nrow(testing))

trainSize = nrow(training)



LCSets = function(x){
  trainIndex <- createDataPartition(training$Outcome, p = x, list = FALSE, times = 1)
  trainLC <- training[ trainIndex,]
  
  modelLC = train(Outcome ~ ., data = trainLC, method = "svmPoly", 
                  tuneGrid = search.grid, trControl = cvCtrl, scaled = TRUE)
  
  trainAccy = modelLC$results$Accuracy
  obs = nrow(trainLC)
  names(obs) = "Obs"
  
  names(trainAccy) = "trainAccy"
  modelPredLC = predict(modelLC, testing)
  output = confusionMatrix(modelPredLC, testing$Outcome)
  testAccy = output$overall[1]

  names(testAccy) = "testAccy"


  return(c(obs, trainAccy, testAccy))
}

results = ldply((1:20)*.05, LCSets)

saveRDS(results, "PO_SVM_poly_LC_results.rds")


```


```{r plotLC}
results = gather(results, type, value, testAccy:trainAccy) %>% group_by(type)

ggplot(results,aes(x=Obs, y = value, color = type, group = type)) + 
  geom_line() + 
  #ggtitle("Decision Tree Learning Curve") +
  ylab("Accuracy") +
  xlab("Observations in Training Set") +
  #scale_y_continuous(labels=comma) +
  theme_bw() +
  guides(fill=guide_legend(title = NULL)) +
  scale_color_discrete(name = "",
    breaks = c("testAccy", "trainAccy"), 
    labels = c("Testing", "Training")) +
  theme(legend.justification = c(1,0), legend.position = c(1,0))

ggsave("PO_SVM_poly_LC.png", width = 4, height = 4, dpi = 120)
dev.off()
```

Calculate execution times

modelLC = train(Outcome ~ ., data = trainLC, method = "svmPoly", 
                  tuneGrid = search.grid, trControl = cvCtrl, scaled = TRUE)

```{r timing, echo=FALSE, warning=FALSE}

degree = modelTune$bestTune$degree
scale = modelTune$bestTune$scale
C = modelTune$bestTune$C

cvCtrl = trainControl(method = "none")  # replace with none
search.grid <- expand.grid(.degree = degree,
                           .scale = scale,
                           .C = C)

trainingTime = microbenchmark(
    train(Outcome ~ ., data = training, method = "svmPoly", 
                    tuneGrid = search.grid, trControl = cvCtrl, scaled = TRUE), times = 100L)

  
testingTime = microbenchmark(
    predict(modelTune, testing), times = 100L)

plot(trainingTime)

trainingTime$time[3]/1000000000
str(trainingTime)
cat("Mean training time:", trainingTime$time[3]/1000000000, "ns")

cat("Mean testing time:", testingTime$time[3]/1000000000, "ns")

```

