all[all == ""] <- NA
#Make the treemap
map.market(id=all$V1, area=all$length, group=all$topic, color=all$sentiment, main="Sentiment/Color, Length/Area, Group/Topic")
setwd("~/Google Drive KB/Open Source Conf")
#libraries
library(openNLP)
library(tm)
#Read in the documents and combine into a single source, then tell R its a string
text<-read.csv('chardonnay2.csv', header=TRUE)
text<-as.String(text$text)
#OpenNLP Annotators
persons <- Maxent_Entity_Annotator(kind = 'person')
locations <- Maxent_Entity_Annotator(kind = 'location')
organizations <- Maxent_Entity_Annotator(kind = 'organization')
sent.token.annotator <- Maxent_Sent_Token_Annotator(language = "en")
word.token.annotator <- Maxent_Word_Token_Annotator(language = "en")
pos.tag.annotator <- Maxent_POS_Tag_Annotator(language = "en")
#annotate and apply
annotations <- annotate(text,
list(sent.token.annotator,word.token.annotator,pos.tag.annotator,
persons,locations,organizations))
#OpenNLP Annotators
persons <- Maxent_Entity_Annotator(kind = 'person')
locations <- Maxent_Entity_Annotator(kind = 'location')
install.packages('openNLPmodels.en')
library(openNLP)
persons <- Maxent_Entity_Annotator(kind = 'person')
library(openNLP.en)
install.packages('openNLP.en')
#Ted Kwartler
#Ted@sportsanalytics.org
#Open Data Science Conference Workshop: Intro to Text Mining using R
#5-30-2015
#v1 Install Packages
install.packages(c('tm', 'stringi','ggplot2','ggthemes','wordcloud','RColorBrewer',
'plyr','stringr','topicmodels','portfolio','openNLP'),
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
install.packages(c("caret", "pROC", "rpart", "gbm", "ggplot2", "lattice",
"kernlab", "partykit", "lubridate", "Fahrmeir", "dplyr"),
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
library(openNLP)
library(tm)
#Read in the documents and combine into a single source, then tell R its a string
text<-read.csv('chardonnay2.csv', header=TRUE)
text<-as.String(text$text)
setwd("~/Google Drive KB/Open Source Conf")
persons <- Maxent_Entity_Annotator(kind = 'person')
install.packages("openNLPmodels.en")
install.packages("openNLP.en")
install.packages("knitr")
install.packages("knitr")
install.packages("microbenchmark")
install.packages("readr")
install.packages("plyr")
install.packages("tidyr")
install.packages("mlbench")
install.packages("vcd")
install.packages("RWeka", dependencies = TRUE)
install.packages("AppliedPredictiveModeling")
install.packages("nnet")
install.packages("neuralnet")
install.packages("RSNNS")
library(openNLP)
library(tm)
#Read in the documents and combine into a single source, then tell R its a string
text<-read.csv('chardonnay2.csv', header=TRUE)
text<-as.String(text$text)
#OpenNLP Annotators
persons <- Maxent_Entity_Annotator(kind = 'person')
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
persons <- Maxent_Entity_Annotator(kind = 'person')
locations <- Maxent_Entity_Annotator(kind = 'location')
organizations <- Maxent_Entity_Annotator(kind = 'organization')
sent.token.annotator <- Maxent_Sent_Token_Annotator(language = "en")
word.token.annotator <- Maxent_Word_Token_Annotator(language = "en")
pos.tag.annotator <- Maxent_POS_Tag_Annotator(language = "en")
#annotate and apply
annotations <- annotate(text,
list(sent.token.annotator,word.token.annotator,pos.tag.annotator,
persons,locations,organizations))
text.annotations<-AnnotatedPlainTextDocument(text,annotations)
#Extract Entities
entities <- function(doc, kind) {
s <- doc$content
a <- annotations(doc)[[1]]
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
s[a[k == kind]]
} else {
s[a[a$type == "entity"]]
}
}
people<-entities(text.annotations, kind = "person")
locations<-entities(text.annotations, kind = "location")
organization<-entities(text.annotations, kind = "organization")
people.head()
head(people)
head(locations)
head(organization)
setwd("~/Documents/ODSC text mining")
#Set the working directory and import libraries
#setwd("~/Google Drive KB/Open Source Conf")
#libraries
library(tm)
library(topicmodels)
#install.packages('topicmodels')
library(portfolio)
#install.packages("portfolio")
#library(ggplot2)
#library(ggthemes)
library(plyr)
library(stringr)
#options, functions
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL','C')
#try to lower function
tryTolower <- function(x){
# return NA when there is an error
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error = function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}
clean.corpus<-function(corpus){
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tryTolower))
corpus <- tm_map(corpus, removeWords, custom.stopwords)
return(corpus)
}
#Bigram token maker
bigram.tokenizer <-function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
#Bring in subjective lexicons
pos <- readLines("positive_words.txt")
neg <-readLines("negative_words.txt")
#Simple sentiment subject word counter function, poached online
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
scores = laply(sentences, function(sentence, pos.words, neg.words) {
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#Create custom stop words
custom.stopwords <- c(stopwords('english'), 'lol', 'smh', 'amp', 'chardonnay')
#bring in some text
text<-read.csv('SFDC_Survey.csv', header=TRUE)
#Create a clean corpus
corpus <- Corpus(DataframeSource(data.frame(text$Experience.Essay)))
corpus <-clean.corpus(corpus)
#Make a DTM
dtm<-DocumentTermMatrix(corpus, control=list(tokenize=bigram.tokenizer))
#In Topic Modeling, remove any docs with all zeros after removing stopwords
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
#In Sentiment, to ensure the number of rows in the dtm.new and the sentiment data frame equal
text <-cbind(text,rowTotals)
text <- text[rowTotals> 0, ]
#Begin Topic Modeling; can use CTM or LDA
topic.model <- LDA(dtm.new, control = list(alpha = 0.1), k = 5)
#Topic Extraction
topics<-get_terms(topic.model, 5)
colnames(topics)<-c("topic1","topic2","topic3","topic4","topic5")
topics<-as.data.frame(topics)
t1<-paste(topics$topic1,collapse=' ')
t2<-paste(topics$topic2,collapse=' ')
t3<-paste(topics$topic3,collapse=' ')
t4<-paste(topics$topic4,collapse=' ')
t5<-paste(topics$topic5,collapse=' ')
topics
#Score each tweet's probability for the topic models then add the topic words to the df as headers
scoring<-posterior(topic.model)
scores<-scoring$topics
scores<-as.data.frame(scores)
colnames(scores)<-c(t1,t2,t3,t4,t5)
#The max probability of each tweet classifies the tweet document
topics.text<-as.data.frame(cbind(row.names(scores),apply(scores,1,function(x) names(scores)[which(x==max(x))])))
#Apply the subjective lexicon scoring function
sentiment.score<-score.sentiment(text$text, pos,neg, .progress='text')
#Get the length of each doc by number of words not characters
doc.length<-rowSums(as.matrix(dtm.new))
#Create a unified data frame
all<-cbind(topics.text,scores,sentiment.score, doc.length)
names(all)[2]<-paste("topic")
names(all)[8]<-paste("sentiment")
names(all)[10]<-paste("length")
all[all == ""] <- NA
#Make the treemap
map.market(id=all$V1, area=all$length, group=all$topic, color=all$sentiment, main="Sentiment/Color, Length/Area, Group/Topic")
#End
#Set the working directory and import libraries
#setwd("~/Google Drive KB/Open Source Conf")
#libraries
library(tm)
library(topicmodels)
#install.packages('topicmodels')
library(portfolio)
#install.packages("portfolio")
#library(ggplot2)
#library(ggthemes)
library(plyr)
library(stringr)
#options, functions
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL','C')
#try to lower function
tryTolower <- function(x){
# return NA when there is an error
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error = function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}
clean.corpus<-function(corpus){
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tryTolower))
corpus <- tm_map(corpus, removeWords, custom.stopwords)
return(corpus)
}
#Bigram token maker
bigram.tokenizer <-function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
#Bring in subjective lexicons
pos <- readLines("positive_words.txt")
neg <-readLines("negative_words.txt")
#Simple sentiment subject word counter function, poached online
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
scores = laply(sentences, function(sentence, pos.words, neg.words) {
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#Create custom stop words
custom.stopwords <- c(stopwords('english'), 'lol', 'smh', 'cusomter service', 'mcgraw hill')
#In Topic Modeling, remove any docs with all zeros after removing stopwords
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
#In Sentiment, to ensure the number of rows in the dtm.new and the sentiment data frame equal
text <-cbind(text,rowTotals)
text <- text[rowTotals> 0, ]
#Begin Topic Modeling; can use CTM or LDA
topic.model <- LDA(dtm.new, control = list(alpha = 0.1), k = 5)
#Topic Extraction
topics<-get_terms(topic.model, 5)
colnames(topics)<-c("topic1","topic2","topic3","topic4","topic5")
topics<-as.data.frame(topics)
t1<-paste(topics$topic1,collapse=' ')
t2<-paste(topics$topic2,collapse=' ')
t3<-paste(topics$topic3,collapse=' ')
t4<-paste(topics$topic4,collapse=' ')
t5<-paste(topics$topic5,collapse=' ')
topics
#bring in some text
text<-read.csv('SFDC_Survey.csv', header=TRUE)
#Create a clean corpus
corpus <- Corpus(DataframeSource(data.frame(text$Experience.Essay)))
corpus <-clean.corpus(corpus)
#Make a DTM
dtm<-DocumentTermMatrix(corpus, control=list(tokenize=bigram.tokenizer))
#In Topic Modeling, remove any docs with all zeros after removing stopwords
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
#In Sentiment, to ensure the number of rows in the dtm.new and the sentiment data frame equal
text <-cbind(text,rowTotals)
text <- text[rowTotals> 0, ]
#Begin Topic Modeling; can use CTM or LDA
topic.model <- LDA(dtm.new, control = list(alpha = 0.1), k = 5)
#Topic Extraction
topics<-get_terms(topic.model, 5)
colnames(topics)<-c("topic1","topic2","topic3","topic4","topic5")
topics<-as.data.frame(topics)
t1<-paste(topics$topic1,collapse=' ')
t2<-paste(topics$topic2,collapse=' ')
t3<-paste(topics$topic3,collapse=' ')
t4<-paste(topics$topic4,collapse=' ')
t5<-paste(topics$topic5,collapse=' ')
topics
#Set the working directory and import libraries
#setwd("~/Google Drive KB/Open Source Conf")
#libraries
library(tm)
library(topicmodels)
#install.packages('topicmodels')
library(portfolio)
#install.packages("portfolio")
#library(ggplot2)
#library(ggthemes)
library(plyr)
library(stringr)
#Create custom stop words
custom.stopwords <- c(stopwords('english'), 'lol', 'smh', 'customer service', 'mcgraw hill')
#bring in some text
text<-read.csv('SFDC_Survey.csv', header=TRUE)
#Create a clean corpus
corpus <- Corpus(DataframeSource(data.frame(text$Experience.Essay)))
corpus <-clean.corpus(corpus)
#Make a DTM
dtm<-DocumentTermMatrix(corpus, control=list(tokenize=bigram.tokenizer))
#In Topic Modeling, remove any docs with all zeros after removing stopwords
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
#In Sentiment, to ensure the number of rows in the dtm.new and the sentiment data frame equal
text <-cbind(text,rowTotals)
text <- text[rowTotals> 0, ]
#Begin Topic Modeling; can use CTM or LDA
topic.model <- LDA(dtm.new, control = list(alpha = 0.1), k = 5)
#Topic Extraction
topics<-get_terms(topic.model, 5)
colnames(topics)<-c("topic1","topic2","topic3","topic4","topic5")
topics<-as.data.frame(topics)
t1<-paste(topics$topic1,collapse=' ')
t2<-paste(topics$topic2,collapse=' ')
t3<-paste(topics$topic3,collapse=' ')
t4<-paste(topics$topic4,collapse=' ')
t5<-paste(topics$topic5,collapse=' ')
topics
#options, functions
options(stringsAsFactors = FALSE)
Sys.setlocale('LC_ALL','C')
#try to lower function
tryTolower <- function(x){
# return NA when there is an error
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error = function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}
clean.corpus<-function(corpus){
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tryTolower))
corpus <- tm_map(corpus, removeWords, custom.stopwords)
return(corpus)
}
#Bigram token maker
bigram.tokenizer <-function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
#Bring in subjective lexicons
pos <- readLines("positive_words.txt")
neg <-readLines("negative_words.txt")
#Simple sentiment subject word counter function, poached online
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
scores = laply(sentences, function(sentence, pos.words, neg.words) {
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#In Topic Modeling, remove any docs with all zeros after removing stopwords
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
#In Sentiment, to ensure the number of rows in the dtm.new and the sentiment data frame equal
text <-cbind(text,rowTotals)
text <- text[rowTotals> 0, ]
#Begin Topic Modeling; can use CTM or LDA
topic.model <- LDA(dtm.new, control = list(alpha = 0.1), k = 5)
#Topic Extraction
topics<-get_terms(topic.model, 5)
colnames(topics)<-c("topic1","topic2","topic3","topic4","topic5")
topics<-as.data.frame(topics)
t1<-paste(topics$topic1,collapse=' ')
t2<-paste(topics$topic2,collapse=' ')
t3<-paste(topics$topic3,collapse=' ')
t4<-paste(topics$topic4,collapse=' ')
t5<-paste(topics$topic5,collapse=' ')
topics
scoring<-posterior(topic.model)
scores<-scoring$topics
scores<-as.data.frame(scores)
colnames(scores)<-c(t1,t2,t3,t4,t5)
#The max probability of each tweet classifies the tweet document
topics.text<-as.data.frame(cbind(row.names(scores),apply(scores,1,function(x) names(scores)[which(x==max(x))])))
sentiment.score<-score.sentiment(text$text, pos,neg, .progress='text')
#Get the length of each doc by number of words not characters
doc.length<-rowSums(as.matrix(dtm.new))
#Create a unified data frame
all<-cbind(topics.text,scores,sentiment.score, doc.length)
dim(topics.text)
dim(scores)
dim(sentiment.score)
dim(doc.length)
head(text$text)
head(pos)
head(neg)
head(text)
sentiment.score<-score.sentiment(text$Experience.Essay, pos, neg, .progress='text')
#Get the length of each doc by number of words not characters
doc.length<-rowSums(as.matrix(dtm.new))
dim(topics.text)
dim(scores)
dim(sentiment.score)
dim(doc.length)
#Create a unified data frame
all<-cbind(topics.text,scores,sentiment.score, doc.length)
names(all)[2]<-paste("topic")
names(all)[8]<-paste("sentiment")
names(all)[10]<-paste("length")
all[all == ""] <- NA
#Make the treemap
map.market(id=all$V1, area=all$length, group=all$topic, color=all$sentiment, main="Sentiment/Color, Length/Area, Group/Topic")
source('~/.active-rstudio-document', echo=TRUE)
setwd("~/Documents/ODSC text mining")
setwd("~/Documents/ODSC text mining")
#libraries
#install.packages("openNLP.en")
#install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
library(openNLP)
library(tm)
#Read in the documents and combine into a single source, then tell R its a string
text<-read.csv('SFDC_Survey.csv', header=TRUE)
text<-as.String(text$Experience.Essay)
#OpenNLP Annotators
persons <- Maxent_Entity_Annotator(kind = 'person')
locations <- Maxent_Entity_Annotator(kind = 'location')
organizations <- Maxent_Entity_Annotator(kind = 'organization')
sent.token.annotator <- Maxent_Sent_Token_Annotator(language = "en")
word.token.annotator <- Maxent_Word_Token_Annotator(language = "en")
pos.tag.annotator <- Maxent_POS_Tag_Annotator(language = "en")
#annotate and apply
annotations <- annotate(text,
list(sent.token.annotator,word.token.annotator,pos.tag.annotator,
persons,locations,organizations))
source('~/.active-rstudio-document', echo=TRUE)
#Apply the subjective lexicon scoring function
sentiment.score<-score.sentiment(text$Experience.Essay, pos, neg, .progress='text')
#Get the length of each doc by number of words not characters
doc.length<-rowSums(as.matrix(dtm.new))
#Create a unified data frame
all<-cbind(topics.text,scores,sentiment.score, doc.length)
names(all)[2]<-paste("topic")
names(all)[8]<-paste("sentiment")
names(all)[10]<-paste("length")
all[all == ""] <- NA
#Make the treemap
map.market(id=all$V1, area=all$length, group=all$topic, color=all$sentiment, main="Sentiment/Color, Length/Area, Group/Topic")
#End
names(all)
head(all)
library(dplyr)
all %>% filter(text <= 0)
all %>% filter(text <= 0)
all %>% filter(text <= -10)
all %>% filter(sentiment.score <= -10)
names(all)
all %>% filter(sentiment <= -10)
all %>% filter(sentiment <= -10)
%>% select(text)
all %>% filter(sentiment <= -10) %>% select(text)
all %>% filter(sentiment <= -10) %>% select(text)
%>% arrange(desc(sentiment))
all %>% filter(sentiment <= -10) %>% select(text) %>% arrange(desc(sentiment))
all %>% filter(sentiment <= -10) %>% arrange(desc(sentiment)) %>% select(text)
all %>% filter(sentiment <= -10) %>% arrange(desc(sentiment)) %>% select(text) %>> summarise(count(text))
all %>% filter(sentiment <= -10) %>% arrange(desc(sentiment)) %>% select(text) %>% summarise(count(text))
all %>% filter(sentiment <= -10) %>% arrange(desc(sentiment)) %>% select(text) %>% summarise(n(text))
all %>% filter(sentiment <= -10) %>% arrange(desc(sentiment)) %>% select(text) %>% summarise(n())
all %>% filter(sentiment <= sent.limit) %>% arrange(desc(sentiment)) %>% select(text) %>% summarise(n())
sent.limit = 0
all %>% filter(sentiment <= sent.limit) %>% arrange(desc(sentiment)) %>% select(text) %>% summarise(n())
boxplot(all.sentiment)
boxplot(all$sentiment)
plot(all$sentiment)
histogram(all$sentiment)
sent.limit = -5
all %>% filter(sentiment <= sent.limit) %>% arrange(desc(sentiment)) %>% select(text)
