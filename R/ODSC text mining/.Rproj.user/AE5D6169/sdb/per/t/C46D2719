{
    "contents" : "---\ntitle: \"SFDC Topic Modeling and Sentiment\"\nauthor: \"Ken Brooks\"\ndate: \"June 3, 2015\"\noutput: pdf_document\n---\n\nAdapted from Ted Kwartler (Ted@sportsanalytics.org), Open Data Science Conference Workshop: Intro to Text Mining using R, 5-30-2015, v7.0 Topic Modeling and simple sentiment\n\n```{r}\n#Set the working directory and import libraries\n#setwd(\"~/Google Drive KB/Open Source Conf\")\n\n#libraries\nlibrary(tm)\nlibrary(topicmodels)\n#install.packages('topicmodels')\nlibrary(portfolio) \n#install.packages(\"portfolio\")\n#library(ggplot2)\n#library(ggthemes)\nlibrary(plyr)\nlibrary(stringr)\nlibrary(dplyr)\n```\n\n\nSet options and defined functions\n---------------------------------\n\n```{r}\n#options, functions\noptions(stringsAsFactors = FALSE)\nSys.setlocale('LC_ALL','C')\n\n#try to lower function\ntryTolower <- function(x){\n  # return NA when there is an error\n  y = NA\n  # tryCatch error\n  try_error = tryCatch(tolower(x), error = function(e) e)\n  # if not an error\n  if (!inherits(try_error, 'error'))\n    y = tolower(x)\n  return(y)\n}\n\nclean.corpus<-function(corpus){\n  corpus <- tm_map(corpus, removePunctuation)\n  corpus <- tm_map(corpus, stripWhitespace)\n  corpus <- tm_map(corpus, removeNumbers)\n  corpus <- tm_map(corpus, content_transformer(tryTolower))\n  corpus <- tm_map(corpus, removeWords, custom.stopwords)\n  return(corpus)\n}\n\n#Bigram token maker\nbigram.tokenizer <-function(x)\n  unlist(lapply(ngrams(words(x), 2), paste, collapse = \" \"), use.names = FALSE)\n\n#Bring in subjective lexicons\npos <- readLines(\"positive_words.txt\")\nneg <-readLines(\"negative_words.txt\")\n\n#Simple sentiment subject word counter function, poached online\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\n{\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\n    word.list = str_split(sentence, '\\\\s+')\n    words = unlist(word.list)\n    # compare our words to the dictionaries of positive & negative terms\n    pos.matches = match(words, pos.words)\n    neg.matches = match(words, neg.words)\n    pos.matches = !is.na(pos.matches)\n    neg.matches = !is.na(neg.matches)\n    #TRUE/FALSE will be treated as 1/0 by sum():\n    score = sum(pos.matches) - sum(neg.matches)\n    return(score)\n  }, pos.words, neg.words, .progress=.progress )\n  scores.df = data.frame(score=scores, text=sentences)\n  return(scores.df)\n}\n\n```\n\nCreate custom stop words\n------------------------\n\n```{r}\n#Create custom stop words\n\ncustom.stopwords <- c(stopwords('english'), 'lol', 'smh', 'customer service', 'mcgraw hill', 'customer support')\n\n```\n\nImport and clean text, build dtm\n--------------------------------\n\n```{r}\n\n#bring in some text\ntext<-read.csv('SFDC_Survey.csv', header=TRUE)\n\n#Create a clean corpus\ncorpus <- Corpus(DataframeSource(data.frame(text$Experience.Essay)))\ncorpus <-clean.corpus(corpus)\n\n#Make a DTM\ndtm<-DocumentTermMatrix(corpus, control=list(tokenize=bigram.tokenizer))\n\n```\n\nPerform topic modeling\n----------------------\n\n```{r}\n\n#In Topic Modeling, remove any docs with all zeros after removing stopwords\nrowTotals <- apply(dtm , 1, sum) \ndtm.new   <- dtm[rowTotals> 0, ]\n\n#In Sentiment, to ensure the number of rows in the dtm.new and the sentiment data frame equal\ntext <-cbind(text,rowTotals)\ntext <- text[rowTotals> 0, ]\n\n#Begin Topic Modeling; can use CTM or LDA\ntopic.model <- LDA(dtm.new, control = list(alpha = 0.1), k = 5) \n\n#Topic Extraction\ntopics<-get_terms(topic.model, 5)\ncolnames(topics)<-c(\"topic1\",\"topic2\",\"topic3\",\"topic4\",\"topic5\")\ntopics<-as.data.frame(topics)\nt1<-paste(topics$topic1,collapse=' ') \nt2<-paste(topics$topic2,collapse=' ') \nt3<-paste(topics$topic3,collapse=' ') \nt4<-paste(topics$topic4,collapse=' ') \nt5<-paste(topics$topic5,collapse=' ') \ntopics\n\n```\n\nAssign documents to topics\n--------------------------\n\n```{r}\n\n#Score each tweet's probability for the topic models then add the topic words to the df as headers\nscoring<-posterior(topic.model)\nscores<-scoring$topics\nscores<-as.data.frame(scores)\ncolnames(scores)<-c(t1,t2,t3,t4,t5)\n\n#The max probability of each tweet classifies the tweet document\ntopics.text<-as.data.frame(cbind(row.names(scores),apply(scores,1,function(x) names(scores)[which(x==max(x))]))) \n\n```\n\nPerform sentiment scoring\n-------------------------\n\n```{r}\n\n#Apply the subjective lexicon scoring function\n\nsentiment.score<-score.sentiment(text$Experience.Essay, pos, neg, .progress='text')\n\n#Get the length of each doc by number of words not characters\ndoc.length<-rowSums(as.matrix(dtm.new))\n\n#Create a unified data frame\nall<-cbind(topics.text,scores,sentiment.score, doc.length)\nnames(all)[2]<-paste(\"topic\")\nnames(all)[8]<-paste(\"sentiment\")\nnames(all)[10]<-paste(\"length\")\nall[all == \"\"] <- NA\n\n#Make the treemap\nmap.market(id=all$V1, area=all$length, group=all$topic, color=all$sentiment, main=\"Sentiment/Color, Length/Area, Group/Topic\")\n\n#End\n```\n\nSort comments with most negative on top and print them\n------------------------------------------------------\n\n```{r}\n\nhistogram(all$sentiment)\n\nsent.limit = -5\n\nall %>% filter(sentiment <= sent.limit) %>% arrange(desc(sentiment)) %>% select(text) \n \n```\n\n\nPlot sentiment over time\n------------------------\n* Make sure data frame is in date order\n* aggregate by week?\n* plot time series\n* add a loess trend line\n",
    "created" : 1433355575646.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1978203781",
    "id" : "C46D2719",
    "lastKnownWriteTime" : 1433379889,
    "path" : "~/Documents/ODSC text mining/7_SFDC_Topic_Modeling_Sentiment.Rmd",
    "project_path" : "7_SFDC_Topic_Modeling_Sentiment.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_markdown"
}